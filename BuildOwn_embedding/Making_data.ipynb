{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af885fe5-bc87-4390-87a6-ec92a9680b65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(20000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 20 seconds\n"
     ]
    }
   ],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "%autosave 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba73c353-d0e9-4ed7-8c3b-6ecc68ffc8de",
   "metadata": {},
   "source": [
    "### Odiya Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0b276fd-3282-4d14-9781-3aea29724574",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115c7f8d-0945-489f-bb0e-5331adc130a4",
   "metadata": {},
   "source": [
    "### Scraping data from Website"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c474373b-bf34-4091-b6ae-7117d7a6630d",
   "metadata": {},
   "source": [
    "# URL of the website\n",
    "url =\"\"\"\n",
    "\n",
    "https://odia.pratilipi.com/read/%E0%AC%AA%E0%AD%8D%E0%AC%B0%E0%AD%87%E0%AC%AE%E0%AC%B0-%E0%AC%A4%E0%AD%80%E0%AC%AC%E0%AD%8D%E0%AC%B0%E0%AC%A4%E0%AC%BE-%E0%AC%AD%E0%AC%BE%E0%AC%97-%E0%AD%A9-%E0%AC%AA%E0%AD%8D%E0%AC%B0%E0%AD%87%E0%AC%AE%E0%AC%B0-%E0%AC%A4%E0%AD%80%E0%AC%AC%E0%AD%8D%E0%AC%B0%E0%AC%A4%E0%AC%BE-%F0%9F%94%A5%E0%AC%AD%E0%AC%BE%E0%AC%97-%E0%AD%A9%F0%9F%92%9C%F0%9F%91%BF-mm3pbtcxrhmd-17ex7461768hl10?redirectTo=%2Fseries%2Fpremer-tibrata-part-2-by-tanaya-sf0iysa45wjd\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "raw",
   "id": "648a93ed-b448-44b7-ad21-bfe67031d6d8",
   "metadata": {},
   "source": [
    "# Send a request to fetch the page content\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the page using BeautifulSoup\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Find all paragraphs\n",
    "odia_paragraphs = soup.find_all('p')\n",
    "\n",
    "# Open the file in append mode\n",
    "with open('data.txt', 'a', encoding='utf-8') as file:\n",
    "    for para in odia_paragraphs:\n",
    "        # Write each Odia paragraph to the file\n",
    "        file.write(para.get_text() + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de06964-fe79-41f3-85bf-1c0a20f30658",
   "metadata": {},
   "source": [
    "### Cleaning and Preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6355a766-3596-4672-987a-add4a8468419",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6e3dfe7-7984-478c-aac7-e0c3fd5f8b2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw text length :158303\n",
      "Cleaned text length :148984\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "148984"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File cleaned successfully.\n"
     ]
    }
   ],
   "source": [
    "# Read the content of the file\n",
    "with open('data.txt', 'r') as file:\n",
    "    text = file.read()\n",
    "    \n",
    "print (f\"Raw text length :{len(text)}\")\n",
    "# Remove all characters except for periods\n",
    "cleaned_text = re.sub(r'[A-Za-z0-9]|://|\\.{2}|[?\\[\\]]|\\.{2}\\/\\/-+|[\\/-]|:|\\.{3,}', '', text)\n",
    "\n",
    "print (f\"Cleaned text length :{len(cleaned_text)}\")\n",
    "\n",
    "# Write the cleaned content to a new file\n",
    "with open('cleaned_data.txt', 'w') as file:\n",
    "    file.write(cleaned_text)\n",
    "\n",
    "print(\"File cleaned successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb6ad41-b0b1-4440-aec1-b278e0d6c973",
   "metadata": {},
   "source": [
    "### Building Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f7c3189-7e03-45e7-a42b-b9294bda5bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "528bb70f-aff3-42c0-88ac-ae9985751749",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer_BPE:\n",
    "    def __init__(self, text, vocab_size=5000):\n",
    "        self.max_vocab_size = vocab_size\n",
    "        self.corpus = text\n",
    "\n",
    "    def _get_token_stats(self, ids):\n",
    "        \"\"\"Compute frequency of adjacent token pairs.\"\"\"\n",
    "        return Counter(zip(ids, ids[1:]))\n",
    "\n",
    "    def _merge_tokens(self, ids, pair, new_idx):\n",
    "        \"\"\"Merge occurrences of the most frequent pair.\"\"\"\n",
    "        i = 0\n",
    "        merged_ids = []\n",
    "        while i < len(ids):\n",
    "            if i < len(ids) - 1 and ids[i] == pair[0] and ids[i + 1] == pair[1]:\n",
    "                merged_ids.append(new_idx)\n",
    "                i += 2\n",
    "            else:\n",
    "                merged_ids.append(ids[i])\n",
    "                i += 1\n",
    "        return merged_ids\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"Train the Byte Pair Encoding tokenizer.\"\"\"\n",
    "        self.vocab = {i: bytes([i]) for i in range(256)}\n",
    "        self.merges = {}\n",
    "        num_merges = self.max_vocab_size - 256\n",
    "\n",
    "        # Encode the corpus into byte tokens.\n",
    "        ids = list(self.corpus.encode(\"utf-8\"))\n",
    "        tokens = ids.copy()\n",
    "        pbar = tqdm(range(num_merges), desc=\"Training BPE Tokenizer\")\n",
    "        for _ in pbar:\n",
    "            stats = self._get_token_stats(ids)\n",
    "            if not stats:\n",
    "                break\n",
    "\n",
    "            # Select the most frequent pair.\n",
    "            most_frequent_pair = max(stats, key=stats.get)\n",
    "\n",
    "            # Assign a new index to the pair and merge.\n",
    "            new_idx = len(self.vocab)\n",
    "            ids = self._merge_tokens(ids, most_frequent_pair, new_idx)\n",
    "\n",
    "            # Update vocab and merges.\n",
    "            self.merges[most_frequent_pair] = new_idx\n",
    "            self.vocab[new_idx] = self.vocab[most_frequent_pair[0]] + self.vocab[most_frequent_pair[1]]\n",
    "            pbar.set_description(f\"Iteration {_}, Compression Ratio {len(tokens) / len(ids):.2f}X\")\n",
    "            if _ in np.arange( 0,num_merges,num_merges/1000):\n",
    "                print(f\"Crossed {len(tokens) / len(ids):.2f} \")\n",
    "                \n",
    "        print(\"++++++++++++++++++++++++++++Final Result ++++++++++++++++++++++++++++\")\n",
    "        print(f\"After training: tokens length: {len(ids)}\")\n",
    "        print(f\"After training: merges length: {len(self.merges)}\")\n",
    "        print(f\"After Training Vocab length {len(self.vocab)}\")\n",
    "        print(f\"compression ratio: {len(tokens) / len(ids):.2f}X\")\n",
    "\n",
    "        return self.vocab, self.merges\n",
    "\n",
    "    def encode(self, text):\n",
    "        \"\"\"Encode text into BPE tokens.\"\"\"\n",
    "        tokens = list(text.encode(\"utf-8\"))\n",
    "        while len(tokens) >= 2:\n",
    "            stats = self._get_token_stats(tokens)\n",
    "            if not stats:\n",
    "                break\n",
    "\n",
    "            # Find the next pair to merge.\n",
    "            pair = min(stats, key=lambda p: self.merges.get(p, float('inf')))\n",
    "            if pair not in self.merges:\n",
    "                break\n",
    "\n",
    "            new_idx = self.merges[pair]\n",
    "            tokens = self._merge_tokens(tokens, pair, new_idx)\n",
    "\n",
    "        return tokens\n",
    "\n",
    "    def decode(self, ids):\n",
    "        \"\"\"Decode BPE tokens back to text.\"\"\"\n",
    "        tokens = b\"\".join(self.vocab[idx] for idx in ids)\n",
    "        return tokens.decode(\"utf-8\", errors=\"replace\")\n",
    "\n",
    "    def save(self, filepath):\n",
    "        \"\"\"Save the tokenizer to a file.\"\"\"\n",
    "        with open(filepath, 'wb') as f:\n",
    "            pickle.dump({\"vocab\": self.vocab, \"merges\": self.merges}, f)\n",
    "        print(f\"Tokenizer saved to {filepath}\")\n",
    "\n",
    "    @staticmethod\n",
    "    def load(filepath):\n",
    "        \"\"\"Load the tokenizer from a file.\"\"\"\n",
    "        with open(filepath, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        tokenizer = Tokenizer_BPE(\"\")\n",
    "        tokenizer.vocab = data[\"vocab\"]\n",
    "        tokenizer.merges = data[\"merges\"]\n",
    "        return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2936c5cb-39ed-4753-aea7-8b71c5e5e5a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration 1, Compression Ratio 1.44X:   0%|                  | 2/4744 [00:00<03:59, 19.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crossed 1.32 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration 609, Compression Ratio 5.56X:  13%|█▊            | 604/4744 [00:10<00:57, 71.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crossed 5.52 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration 1202, Compression Ratio 6.77X:  25%|███         | 1198/4744 [00:18<00:43, 82.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crossed 6.74 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration 1797, Compression Ratio 7.60X:  38%|████▌       | 1792/4744 [00:25<00:34, 85.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crossed 7.57 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration 2391, Compression Ratio 8.27X:  50%|██████      | 2384/4744 [00:32<00:26, 89.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crossed 8.25 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration 2986, Compression Ratio 8.89X:  63%|███████▌    | 2982/4744 [00:38<00:17, 98.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crossed 8.87 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration 3581, Compression Ratio 9.43X:  75%|████████▎  | 3580/4744 [00:44<00:10, 107.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crossed 9.41 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration 4175, Compression Ratio 9.97X:  88%|█████████▋ | 4168/4744 [00:50<00:05, 110.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crossed 9.96 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration 4743, Compression Ratio 10.43X: 100%|███████████| 4744/4744 [00:54<00:00, 86.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++++++++++++++++++Final Result ++++++++++++++++++++++++++++\n",
      "After training: tokens length: 37291\n",
      "After training: merges length: 4744\n",
      "After Training Vocab length 5000\n",
      "compression ratio: 10.43X\n",
      "Tokenizer saved to odiya_tokenizer.pkl\n",
      "Odiys Text Example : ପ୍ରଥମ ବିଶ୍ୱ ଓଡିଆ ଭାଷା ସମ୍ମିଳନୀର ଐତିହାସିକ ଦିଗ  ଓଡିଆ ଭାଷାର ପ୍ରଚାର, ପ୍ରସାର ତଥା ଗବେଷଣାକୁ ପ୍ରାଧାନ୍ୟ ଦେବା ପ୍ରଥମ ବିଶ୍ୱ ଓଡିଆ ଭାଷା ସମ୍ମିଳନୀର ପ୍ରଧାନ ଲକ୍ଷ୍ୟ । ଏହି ସମ୍ମିଳନୀ ରାଜ୍ୟ, ଜାତୀୟ ଓ ଅନ୍ତର୍ଜାତୀୟ ସ୍ତରରେ ଆୟୋଜନ\n",
      "Encoded Text: [4310, 3371, 310, 4312, 369, 830, 32, 1460, 281, 365, 2032, 1260, 3372, 2243, 151, 303, 1048, 304, 338, 467, 2919, 302, 1178, 1179, 759, 3371, 266, 1385, 4313, 1180, 362, 1339, 1633, 2506, 3373, 2920, 4314, 1300, 3375, 276] \n",
      "\n",
      "Decoded the Tokens: ପ୍ରଥମ ବିଶ୍ୱ ଓଡିଆ ଭାଷା ସମ୍ମିଳନୀର ଐତିହାସିକ ଦିଗ  ଓଡିଆ ଭାଷାର ପ୍ରଚାର, ପ୍ରସାର ତଥା ଗବେଷଣାକୁ ପ୍ରାଧାନ୍ୟ ଦେବା ପ୍ରଥମ ବିଶ୍ୱ ଓଡିଆ ଭାଷା ସମ୍ମିଳନୀର ପ୍ରଧାନ ଲକ୍ଷ୍ୟ । ଏହି ସମ୍ମିଳନୀ ରାଜ୍ୟ, ଜାତୀୟ ଓ ଅନ୍ତର୍ଜାତୀୟ ସ୍ତରରେ ଆୟୋଜନ \n",
      "\n",
      "Tokenizer Working Fine !!  <3 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    with open('cleaned_data.txt', 'r') as file:\n",
    "        data = file.read()\n",
    "    example_text = data[:200]\n",
    "    \n",
    "    max_vocab_size = 5000\n",
    "    odiya_tokenizer = Tokenizer_BPE(data,max_vocab_size)\n",
    "    vocab, merges = odiya_tokenizer.train()\n",
    "    odiya_tokenizer.save(\"odiya_tokenizer.pkl\")\n",
    "\n",
    "    loaded_tokenizer = Tokenizer_BPE.load(\"odiya_tokenizer.pkl\")\n",
    "    print(f\"Odiys Text Example : {example_text}\")\n",
    "    encoded = loaded_tokenizer.encode(example_text)\n",
    "    decoded = loaded_tokenizer.decode(encoded)\n",
    "    print(f\"Encoded Text: {encoded} \\n\")\n",
    "    print(f\"Decoded the Tokens: {decoded} \\n\")\n",
    "    \n",
    "    if decoded == example_text:\n",
    "        print(\"Tokenizer Working Fine !!  <3 \\n\")\n",
    "    else:\n",
    "        print(\"Tokenizer Not Working Well. Please Chcek the Steps \\n\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44fcf95-dcbb-43d6-ad23-7dbd870e9059",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1002859d-06d5-4619-9e22-4ef3d0e5dc8d",
   "metadata": {},
   "source": [
    "### Making Gradio APP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da48bb7d-9e51-49e2-96c7-0ac5766bad5f",
   "metadata": {},
   "source": [
    "##### Example App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7e08940-cf85-4863-86b4-17658735cce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jd/miniconda3/envs/era_n/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "# Define a function for your app\n",
    "def reverse_text(input_text):\n",
    "    return input_text[::-1]\n",
    "\n",
    "# Create the Gradio interface\n",
    "interface = gr.Interface(\n",
    "    fn=reverse_text,         # Function to run\n",
    "    inputs=\"text\",           # Input component\n",
    "    outputs=\"text\",          # Output component\n",
    "    title=\"Text Reverser\",   # App title\n",
    "    description=\"Enter text to reverse it.\",  # App description\n",
    ")\n",
    "\n",
    "# Launch the app\n",
    "interface.launch()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aed502b-83f2-4c84-9041-b281a9b9e5ab",
   "metadata": {},
   "source": [
    "### Let's Build the Tokenizer app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b41717b1-eba4-4a3b-8fec-7fce28726e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Odiya_Tokenizer import Tokenizer_BPE\n",
    "import pickle\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "91fcba53-f86f-4598-8d85-82905344a27e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7862\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7862/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the TextProcessor class\n",
    "class TextProcessor:\n",
    "    def __init__(self, tokenizer_path):\n",
    "        self.loaded_tokenizer = Tokenizer_BPE.load(tokenizer_path)\n",
    "        \n",
    "    def encode(self, example_text):\n",
    "        encoded_text = self.loaded_tokenizer.encode(example_text)\n",
    "        return str(encoded_text)  # Convert to string for Gradio output\n",
    "\n",
    "    def decode(self, encoded_text):\n",
    "        decoded_text = self.loaded_tokenizer.decode(eval(encoded_text))  # Convert the input string back to a list\n",
    "        return decoded_text\n",
    "\n",
    "# Instantiate the TextProcessor with the tokenizer path\n",
    "tokenizer = TextProcessor(\"odiya_tokenizer.pkl\")\n",
    "\n",
    "# Define the Gradio app layout\n",
    "def beautify_app():\n",
    "    \n",
    "    with gr.Blocks(css=\"\"\"\n",
    "        #encode-header, #decode-header {\n",
    "            font-size: 22px;\n",
    "            font-weight: bold;\n",
    "            color: #2D87D6;\n",
    "            text-align: center;\n",
    "        }\n",
    "        #input-textbox, #token-input {\n",
    "            border-radius: 10px;\n",
    "            border: 2px solid #2D87D6;\n",
    "            background-color: #E9F2FB;\n",
    "            padding: 12px;\n",
    "            margin-bottom: 10px;\n",
    "            font-size: 16px;\n",
    "            width: 100%;\n",
    "        }\n",
    "        #encoded-output, #decoded-output {\n",
    "            border-radius: 10px;\n",
    "            border: 2px solid #2D87D6;\n",
    "            background-color: #E9F2FB;\n",
    "            padding: 12px;\n",
    "            font-size: 16px;\n",
    "            width: 100%;\n",
    "        }\n",
    "        #encode-btn, #decode-btn {\n",
    "            background-color: #2D87D6;\n",
    "            color: white;\n",
    "            font-weight: bold;\n",
    "            border-radius: 12px;\n",
    "            border: none;\n",
    "            padding: 12px;\n",
    "            width: 100%;\n",
    "            font-size: 16px;\n",
    "            transition: background-color 0.3s ease;\n",
    "        }\n",
    "        #encode-btn:hover, #decode-btn:hover {\n",
    "            background-color: #1C6BB2;\n",
    "        }\n",
    "        .gr-button {\n",
    "            margin-top: 15px;\n",
    "        }\n",
    "    \"\"\") as app:\n",
    "        gr.Markdown(\n",
    "        \"\"\"\n",
    "        <h1 style=\"text-align: center; font-size: 2.5em;\">ଏହା ଏକ ଓଡିଆ ଟୋକେନାଇଜର୍ ଆପ୍| {This is a Odiya tokenizer app} Copy text in Encoder to see the Tokens.</h1>\n",
    "        <p>Odiya Tokenizer (BPE Encoding and Decoding)</p>\n",
    "        \"\"\",\n",
    "        elem_id=\"title\"\n",
    "        )\n",
    "        \n",
    "        with gr.Row():\n",
    "            # Left Column: Encode Text\n",
    "            with gr.Column(scale=1, min_width=400):\n",
    "                gr.Markdown(\"### **Encode Text**\", elem_id=\"encode-header\")\n",
    "                input_text = gr.Textbox(\n",
    "                    label=\"Enter Odiya Text\", \n",
    "                    lines=10, \n",
    "                    placeholder=\"ଆମେ ସମସ୍ତେ ଭାରତୀୟ। କିନ୍ତୁ ଆମେ ପ୍ରଥମ ମଣିଷ |\",\n",
    "                    elem_id=\"input-textbox\"\n",
    "                )\n",
    "                encode_button = gr.Button(\"Encode\", elem_id=\"encode-btn\")\n",
    "                encoded_output = gr.Textbox(\n",
    "                    label=\"Encoded Tokens\", \n",
    "                    lines=10, \n",
    "                    interactive=False, \n",
    "                    placeholder=\"Encoded tokens will appear here.\",\n",
    "                    elem_id=\"encoded-output\"\n",
    "                )\n",
    "\n",
    "            # Right Column: Decode Tokens\n",
    "            with gr.Column(scale=1, min_width=400):\n",
    "                gr.Markdown(\"### **Decode Tokens**\", elem_id=\"decode-header\")\n",
    "                token_input = gr.Textbox(\n",
    "                    label=\"Enter Encoded Tokens (comma-separated)\", \n",
    "                    lines=10, \n",
    "                    placeholder=\"Example: [256, 474, 4786, 1501, 763, 607, 3672, 474, 4707, 300, 1858, 1326]\",\n",
    "                    elem_id=\"token-input\"\n",
    "                )\n",
    "                decode_button = gr.Button(\"Decode\", elem_id=\"decode-btn\")\n",
    "                decoded_output = gr.Textbox(\n",
    "                    label=\"Decoded Text\", \n",
    "                    lines=10, \n",
    "                    interactive=False, \n",
    "                    placeholder=\"Decoded text will appear here.\",\n",
    "                    elem_id=\"decoded-output\"\n",
    "                )\n",
    "\n",
    "        # Function calls when buttons are clicked\n",
    "        encode_button.click(fn=tokenizer.encode, inputs=input_text, outputs=encoded_output)\n",
    "        decode_button.click(fn=tokenizer.decode, inputs=token_input, outputs=decoded_output)\n",
    "\n",
    "    return app\n",
    "\n",
    "# Running the app\n",
    "app = beautify_app()\n",
    "app.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661ce0c2-8d4c-4b88-a887-0914fdaea3b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
